{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전치행렬과 max 값을 이용하여 데이터 Normalize 한 후,\n",
    "## training data 와 test data 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 수치미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# sigmoid 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wine Class\n",
    "\n",
    "class Wine:\n",
    "    \n",
    "    # 생성자\n",
    "    # xdata, tdata => numpy.array(...)\n",
    "    def __init__(self, name, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        # 2층 hidden layer unit \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.W2 = np.random.rand(input_nodes, hidden_nodes)  \n",
    "        self.b2 = np.random.rand(hidden_nodes)\n",
    "        \n",
    "        # 3층 output layer unit : 1 개 \n",
    "        self.W3 = np.random.rand(hidden_nodes,output_nodes)\n",
    "        self.b3 = np.random.rand(output_nodes)\n",
    "                        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(self.name, \" is created !!!\")\n",
    "        \n",
    "    # 손실함수\n",
    "    def feed_forward(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # obtain W and b\n",
    "    def get_W_b(self):\n",
    "        \n",
    "        return self.W2,  self.b2, self.W3, self.b3\n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z2 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, input_data):    \n",
    "        \n",
    "        z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        if y >= 0.5:\n",
    "            result = 1  # True\n",
    "        else:\n",
    "            result = 0  # False\n",
    "    \n",
    "        return y, result\n",
    "\n",
    "    \n",
    "    def accuracy(self, input_data, target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        # list which contains (index, label, prediction) value\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        # temp list which contains label and prediction in sequence\n",
    "        temp_list = []\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "            \n",
    "            (real_val, logical_val) = self.predict(input_data[index])\n",
    "            \n",
    "            if logical_val == target_data[index]:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "                temp_list.append(index)\n",
    "                temp_list.append(target_data[index])\n",
    "                temp_list.append(logical_val)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []\n",
    "                \n",
    "                \n",
    "        accuracy_result = len(matched_list) / len(input_data)\n",
    "        \n",
    "        print(\"Accuracy => \", accuracy_result)\n",
    "        \n",
    "        return matched_list, not_matched_list, index_label_prediction_list\n",
    "    \n",
    "        \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self, input_data, target_data):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n",
    "    \n",
    "        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n",
    "        \n",
    "        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n",
    "    \n",
    "        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wine.csv 를 읽어 온 후에,\n",
    "## 각 열의 최대값을 구하기 위해 원본 행렬을 전치행렬로 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_data.shape =  (6497, 13)\n",
      "transpose_loaded_data.shape =  (13, 6497)\n"
     ]
    }
   ],
   "source": [
    "# Normalize 않되어 있는 ThoracicSurgery.csv 읽어옴\n",
    "loaded_data = np.loadtxt('./wine.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print(\"loaded_data.shape = \", loaded_data.shape)\n",
    "\n",
    "# 각 열의 최대값을 찾기 위해 행과 열을 바꾸어 줌. 즉 전치향렬을 만들어줌\n",
    "transpose_loaded_data = loaded_data.T\n",
    "\n",
    "print(\"transpose_loaded_data.shape = \", transpose_loaded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전치된 행렬에서 각 행의 최대 값을 찾아 나누어 주면서 Normalize 시킨 후에,\n",
    "## 다시 전치행렬을 만들어서 csv 파일로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 6497)\n",
      "(6497, 13)\n"
     ]
    }
   ],
   "source": [
    "# 전치행렬을 위한 리스트\n",
    "transpose_normalize_data_list = []\n",
    "\n",
    "for index in range(len(transpose_loaded_data)):\n",
    "    \n",
    "    max_value = np.max(transpose_loaded_data[index, :])   # 각 행의 최대값을 찾음\n",
    "    \n",
    "    # 최대값이 1 이상이면 최대값으로 나누어서 \n",
    "    # 모든 데이터가 0 ~ 1 사이에 오도록 함\n",
    "    if max_value > 1.0:  \n",
    "        \n",
    "        transpose_normalize_data_list.append(transpose_loaded_data[index, :] / max_value)\n",
    "       \n",
    "    # 최대값이 1 보다 작으면 해당 값을 그대로 사용함. \n",
    "    ## 왜냐하면 1보다 작은 값이면 굳이 바꿀 필요가 없음\n",
    "    else:      \n",
    "        \n",
    "        transpose_normalize_data_list.append(transpose_loaded_data[index, :])\n",
    "        \n",
    "\n",
    "# 리스트를 numpy type 으로 변환\n",
    "transpose_normalize_data = np.array(transpose_normalize_data_list)\n",
    "\n",
    "print(transpose_normalize_data.shape)\n",
    "\n",
    "# 데이터 저장을 위해 다시 전치행렬을 통해 행과 열을 바꿈\n",
    "normalize_data = transpose_normalize_data.T\n",
    "\n",
    "print(normalize_data.shape)\n",
    "\n",
    "# normalize 파일저장\n",
    "np.savetxt('./Normalize_Wine_data.csv', normalize_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_data.shape =  (6497, 13)\n",
      "total_data_num =  6497 , test_data_num =  2598\n",
      "length of test_data_index_list =  2598\n",
      "training_data.shape =  (3899, 13)\n",
      "test_data.shape =  (2598, 13)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "loaded_data = np.loadtxt('./Normalize_Wine_data.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print(\"loaded_data.shape = \", loaded_data.shape)\n",
    "\n",
    "# 임시 저장 리스트\n",
    "training_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# 전체의 40%를 테스트데이터로 분리\n",
    "total_data_num = len(loaded_data)\n",
    "seperation_rate = 0.4\n",
    "test_data_num = int(len(loaded_data) * seperation_rate)\n",
    "\n",
    "print(\"total_data_num = \", total_data_num, \", test_data_num = \", test_data_num)\n",
    "\n",
    "# 전체 데이터 인덱스를 가지고 있는 리스트 생성\n",
    "total_data_index_list = [ index for index in range(total_data_num) ]\n",
    "\n",
    "# random.sample 을 이용하여 테스트 데이터를 위한 인덱스 리스트 생성\n",
    "test_data_index_list = random.sample(range(total_data_num), test_data_num)\n",
    "\n",
    "print(\"length of test_data_index_list = \", len(test_data_index_list))\n",
    "\n",
    "\n",
    "# training data 는 전체 데이터에서 test_data 를 위한 index 를 제외하면 됨\n",
    "for index in range(len(test_data_index_list)):\n",
    "    \n",
    "    total_data_index_list.remove(test_data_index_list[index])\n",
    "    \n",
    "\n",
    "# training data 구성\n",
    "for training_data_index in total_data_index_list:\n",
    "    \n",
    "    training_data_list.append(loaded_data[training_data_index])\n",
    "\n",
    "# test data 구성\n",
    "for test_data_index in test_data_index_list:\n",
    "    \n",
    "    test_data_list.append(loaded_data[test_data_index])\n",
    "\n",
    "# generate training data from training_data_list using np.arrya(...)\n",
    "training_data = np.array(training_data_list)\n",
    "\n",
    "# generate test data from test_data_list using np.arrya(...)\n",
    "test_data = np.array(test_data_list)\n",
    "\n",
    "# verification shape\n",
    "print(\"training_data.shape = \", training_data.shape)\n",
    "print(\"test_data.shape = \", test_data.shape)\n",
    "\n",
    "# save training & test data (.csv)\n",
    "np.savetxt('./random_wine_normalized_training_data.csv', training_data, delimiter=',')\n",
    "np.savetxt('./random_wine_normalized_test_data.csv', test_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINE  is created !!!\n",
      "Neural Network Learning using Numerical Derivative...\n",
      "epochs =  0 loss value =  2.334120973349792\n",
      "epochs =  1 loss value =  1.2862780806935439\n",
      "epochs =  2 loss value =  0.7377022868902284\n",
      "epochs =  3 loss value =  0.4964511011799369\n",
      "epochs =  4 loss value =  0.3847193163557317\n",
      "epochs =  5 loss value =  0.32707661809195954\n",
      "epochs =  6 loss value =  0.29462388358582425\n",
      "epochs =  7 loss value =  0.27521882852835516\n",
      "epochs =  8 loss value =  0.26314109204380104\n",
      "epochs =  9 loss value =  0.2554216339533987\n",
      "epochs =  10 loss value =  0.2503999321992277\n",
      "epochs =  11 loss value =  0.24709444903403582\n",
      "epochs =  12 loss value =  0.24490132313067067\n",
      "epochs =  13 loss value =  0.24343835869409727\n",
      "epochs =  14 loss value =  0.24245881123124885\n",
      "epochs =  15 loss value =  0.24180118296610997\n",
      "epochs =  16 loss value =  0.24135877152261803\n",
      "epochs =  17 loss value =  0.2410606237566577\n",
      "epochs =  18 loss value =  0.24085935197572905\n",
      "epochs =  19 loss value =  0.24072321340892824\n",
      "\n",
      "Elapsed Time =>  0:05:48.541000\n"
     ]
    }
   ],
   "source": [
    "#hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 5  # hidden nodes 개수. \n",
    "o_nodes = 1    # output nodes 개수\n",
    "lr = 1e-4      # learning rate. hi_node = 1, 1e-1, 1e-2 에서는 발산\n",
    "epochs = 20   # 반복횟수. \n",
    "\n",
    "# Wine 객체 생성\n",
    "obj = Wine(\"WINE\", i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "print(\"Neural Network Learning using Numerical Derivative...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in range(epochs):\n",
    "    \n",
    "    for index in range(len(training_data)):\n",
    "        \n",
    "        input_data = training_data[index, 0:-1]\n",
    "        target_data = training_data[index, [-1]]\n",
    "        \n",
    "        obj.train(input_data, target_data)\n",
    "        \n",
    "    print(\"epochs = \", step, \"loss value = \", obj.loss_val())\n",
    "\n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.shape =  (2598, 13)\n",
      "Accuracy =>  0.7548113933795227\n"
     ]
    }
   ],
   "source": [
    "test_data = np.loadtxt('./random_wine_normalized_test_data.csv', delimiter=',', dtype=np.float32)\n",
    "print(\"test_data.shape = \", test_data.shape)\n",
    "\n",
    "test_input_data = test_data[ :, 0:-1 ]\n",
    "test_target_data = test_data[ :, -1 ]\n",
    "\n",
    "(true_list_1, false_list_1, index_label_prediction_list) = obj.accuracy(test_input_data, test_target_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1.0, 0], [1, 1.0, 0], [5, 1.0, 0], [8, 1.0, 0], [20, 1.0, 0], [25, 1.0, 0], [28, 1.0, 0], [34, 1.0, 0], [38, 1.0, 0], [39, 1.0, 0], [40, 1.0, 0], [41, 1.0, 0], [43, 1.0, 0], [48, 1.0, 0], [54, 1.0, 0], [58, 1.0, 0], [59, 1.0, 0], [61, 1.0, 0], [65, 1.0, 0], [66, 1.0, 0], [68, 1.0, 0], [72, 1.0, 0], [73, 1.0, 0], [74, 1.0, 0], [79, 1.0, 0], [83, 1.0, 0], [91, 1.0, 0], [95, 1.0, 0], [96, 1.0, 0], [97, 1.0, 0], [105, 1.0, 0], [106, 1.0, 0], [107, 1.0, 0], [116, 1.0, 0], [119, 1.0, 0], [123, 1.0, 0], [126, 1.0, 0], [131, 1.0, 0], [132, 1.0, 0], [141, 1.0, 0], [143, 1.0, 0], [155, 1.0, 0], [157, 1.0, 0], [164, 1.0, 0], [166, 1.0, 0], [175, 1.0, 0], [177, 1.0, 0], [178, 1.0, 0], [180, 1.0, 0], [187, 1.0, 0], [188, 1.0, 0], [191, 1.0, 0], [205, 1.0, 0], [211, 1.0, 0], [219, 1.0, 0], [220, 1.0, 0], [223, 1.0, 0], [227, 1.0, 0], [229, 1.0, 0], [240, 1.0, 0], [241, 1.0, 0], [242, 1.0, 0], [243, 1.0, 0], [244, 1.0, 0], [251, 1.0, 0], [254, 1.0, 0], [255, 1.0, 0], [256, 1.0, 0], [258, 1.0, 0], [260, 1.0, 0], [268, 1.0, 0], [269, 1.0, 0], [270, 1.0, 0], [271, 1.0, 0], [272, 1.0, 0], [274, 1.0, 0], [279, 1.0, 0], [280, 1.0, 0], [281, 1.0, 0], [289, 1.0, 0], [292, 1.0, 0], [296, 1.0, 0], [300, 1.0, 0], [305, 1.0, 0], [306, 1.0, 0], [307, 1.0, 0], [310, 1.0, 0], [319, 1.0, 0], [322, 1.0, 0], [324, 1.0, 0], [326, 1.0, 0], [329, 1.0, 0], [335, 1.0, 0], [336, 1.0, 0], [342, 1.0, 0], [350, 1.0, 0], [361, 1.0, 0], [367, 1.0, 0], [379, 1.0, 0], [382, 1.0, 0], [385, 1.0, 0], [387, 1.0, 0], [388, 1.0, 0], [389, 1.0, 0], [392, 1.0, 0], [394, 1.0, 0], [395, 1.0, 0], [398, 1.0, 0], [402, 1.0, 0], [406, 1.0, 0], [409, 1.0, 0], [411, 1.0, 0], [415, 1.0, 0], [417, 1.0, 0], [418, 1.0, 0], [425, 1.0, 0], [426, 1.0, 0], [428, 1.0, 0], [429, 1.0, 0], [430, 1.0, 0], [438, 1.0, 0], [442, 1.0, 0], [443, 1.0, 0], [454, 1.0, 0], [456, 1.0, 0], [457, 1.0, 0], [463, 1.0, 0], [466, 1.0, 0], [467, 1.0, 0], [468, 1.0, 0], [473, 1.0, 0], [478, 1.0, 0], [489, 1.0, 0], [498, 1.0, 0], [500, 1.0, 0], [504, 1.0, 0], [524, 1.0, 0], [527, 1.0, 0], [540, 1.0, 0], [542, 1.0, 0], [556, 1.0, 0], [557, 1.0, 0], [559, 1.0, 0], [563, 1.0, 0], [571, 1.0, 0], [575, 1.0, 0], [581, 1.0, 0], [591, 1.0, 0], [601, 1.0, 0], [604, 1.0, 0], [605, 1.0, 0], [609, 1.0, 0], [611, 1.0, 0], [637, 1.0, 0], [640, 1.0, 0], [642, 1.0, 0], [646, 1.0, 0], [648, 1.0, 0], [649, 1.0, 0], [651, 1.0, 0], [654, 1.0, 0], [663, 1.0, 0], [666, 1.0, 0], [669, 1.0, 0], [670, 1.0, 0], [674, 1.0, 0], [679, 1.0, 0], [682, 1.0, 0], [684, 1.0, 0], [688, 1.0, 0], [690, 1.0, 0], [692, 1.0, 0], [694, 1.0, 0], [695, 1.0, 0], [699, 1.0, 0], [702, 1.0, 0], [703, 1.0, 0], [704, 1.0, 0], [709, 1.0, 0], [714, 1.0, 0], [718, 1.0, 0], [719, 1.0, 0], [724, 1.0, 0], [727, 1.0, 0], [728, 1.0, 0], [731, 1.0, 0], [732, 1.0, 0], [743, 1.0, 0], [751, 1.0, 0], [757, 1.0, 0], [758, 1.0, 0], [759, 1.0, 0], [760, 1.0, 0], [763, 1.0, 0], [767, 1.0, 0], [772, 1.0, 0], [778, 1.0, 0], [781, 1.0, 0], [782, 1.0, 0], [789, 1.0, 0], [790, 1.0, 0], [792, 1.0, 0], [793, 1.0, 0], [800, 1.0, 0], [801, 1.0, 0], [819, 1.0, 0], [825, 1.0, 0], [831, 1.0, 0], [832, 1.0, 0], [835, 1.0, 0], [845, 1.0, 0], [847, 1.0, 0], [849, 1.0, 0], [853, 1.0, 0], [854, 1.0, 0], [857, 1.0, 0], [877, 1.0, 0], [883, 1.0, 0], [894, 1.0, 0], [896, 1.0, 0], [898, 1.0, 0], [899, 1.0, 0], [901, 1.0, 0], [903, 1.0, 0], [904, 1.0, 0], [905, 1.0, 0], [906, 1.0, 0], [908, 1.0, 0], [910, 1.0, 0], [914, 1.0, 0], [917, 1.0, 0], [919, 1.0, 0], [922, 1.0, 0], [928, 1.0, 0], [930, 1.0, 0], [931, 1.0, 0], [932, 1.0, 0], [935, 1.0, 0], [937, 1.0, 0], [943, 1.0, 0], [944, 1.0, 0], [948, 1.0, 0], [951, 1.0, 0], [952, 1.0, 0], [955, 1.0, 0], [956, 1.0, 0], [958, 1.0, 0], [962, 1.0, 0], [963, 1.0, 0], [979, 1.0, 0], [982, 1.0, 0], [985, 1.0, 0], [988, 1.0, 0], [993, 1.0, 0], [994, 1.0, 0], [995, 1.0, 0], [1000, 1.0, 0], [1002, 1.0, 0], [1004, 1.0, 0], [1016, 1.0, 0], [1019, 1.0, 0], [1023, 1.0, 0], [1032, 1.0, 0], [1033, 1.0, 0], [1034, 1.0, 0], [1036, 1.0, 0], [1037, 1.0, 0], [1041, 1.0, 0], [1048, 1.0, 0], [1053, 1.0, 0], [1059, 1.0, 0], [1067, 1.0, 0], [1074, 1.0, 0], [1076, 1.0, 0], [1077, 1.0, 0], [1080, 1.0, 0], [1090, 1.0, 0], [1093, 1.0, 0], [1108, 1.0, 0], [1114, 1.0, 0], [1117, 1.0, 0], [1120, 1.0, 0], [1124, 1.0, 0], [1133, 1.0, 0], [1134, 1.0, 0], [1136, 1.0, 0], [1138, 1.0, 0], [1144, 1.0, 0], [1150, 1.0, 0], [1154, 1.0, 0], [1157, 1.0, 0], [1160, 1.0, 0], [1170, 1.0, 0], [1171, 1.0, 0], [1180, 1.0, 0], [1185, 1.0, 0], [1193, 1.0, 0], [1194, 1.0, 0], [1202, 1.0, 0], [1209, 1.0, 0], [1213, 1.0, 0], [1220, 1.0, 0], [1223, 1.0, 0], [1224, 1.0, 0], [1226, 1.0, 0], [1227, 1.0, 0], [1229, 1.0, 0], [1232, 1.0, 0], [1240, 1.0, 0], [1243, 1.0, 0], [1249, 1.0, 0], [1260, 1.0, 0], [1266, 1.0, 0], [1274, 1.0, 0], [1275, 1.0, 0], [1287, 1.0, 0], [1289, 1.0, 0], [1290, 1.0, 0], [1306, 1.0, 0], [1312, 1.0, 0], [1314, 1.0, 0], [1316, 1.0, 0], [1318, 1.0, 0], [1324, 1.0, 0], [1326, 1.0, 0], [1328, 1.0, 0], [1329, 1.0, 0], [1330, 1.0, 0], [1337, 1.0, 0], [1339, 1.0, 0], [1345, 1.0, 0], [1351, 1.0, 0], [1352, 1.0, 0], [1360, 1.0, 0], [1362, 1.0, 0], [1367, 1.0, 0], [1370, 1.0, 0], [1373, 1.0, 0], [1375, 1.0, 0], [1378, 1.0, 0], [1387, 1.0, 0], [1390, 1.0, 0], [1391, 1.0, 0], [1392, 1.0, 0], [1399, 1.0, 0], [1400, 1.0, 0], [1404, 1.0, 0], [1407, 1.0, 0], [1408, 1.0, 0], [1410, 1.0, 0], [1415, 1.0, 0], [1418, 1.0, 0], [1419, 1.0, 0], [1426, 1.0, 0], [1430, 1.0, 0], [1437, 1.0, 0], [1440, 1.0, 0], [1442, 1.0, 0], [1443, 1.0, 0], [1447, 1.0, 0], [1448, 1.0, 0], [1449, 1.0, 0], [1472, 1.0, 0], [1479, 1.0, 0], [1484, 1.0, 0], [1485, 1.0, 0], [1486, 1.0, 0], [1489, 1.0, 0], [1494, 1.0, 0], [1498, 1.0, 0], [1499, 1.0, 0], [1511, 1.0, 0], [1519, 1.0, 0], [1520, 1.0, 0], [1523, 1.0, 0], [1524, 1.0, 0], [1529, 1.0, 0], [1537, 1.0, 0], [1551, 1.0, 0], [1557, 1.0, 0], [1563, 1.0, 0], [1565, 1.0, 0], [1574, 1.0, 0], [1576, 1.0, 0], [1578, 1.0, 0], [1579, 1.0, 0], [1580, 1.0, 0], [1589, 1.0, 0], [1590, 1.0, 0], [1595, 1.0, 0], [1596, 1.0, 0], [1602, 1.0, 0], [1621, 1.0, 0], [1623, 1.0, 0], [1627, 1.0, 0], [1633, 1.0, 0], [1635, 1.0, 0], [1636, 1.0, 0], [1638, 1.0, 0], [1639, 1.0, 0], [1642, 1.0, 0], [1648, 1.0, 0], [1653, 1.0, 0], [1655, 1.0, 0], [1656, 1.0, 0], [1658, 1.0, 0], [1664, 1.0, 0], [1666, 1.0, 0], [1671, 1.0, 0], [1672, 1.0, 0], [1684, 1.0, 0], [1685, 1.0, 0], [1686, 1.0, 0], [1689, 1.0, 0], [1690, 1.0, 0], [1693, 1.0, 0], [1695, 1.0, 0], [1699, 1.0, 0], [1701, 1.0, 0], [1702, 1.0, 0], [1704, 1.0, 0], [1712, 1.0, 0], [1714, 1.0, 0], [1725, 1.0, 0], [1735, 1.0, 0], [1742, 1.0, 0], [1743, 1.0, 0], [1749, 1.0, 0], [1750, 1.0, 0], [1753, 1.0, 0], [1757, 1.0, 0], [1760, 1.0, 0], [1765, 1.0, 0], [1774, 1.0, 0], [1778, 1.0, 0], [1780, 1.0, 0], [1781, 1.0, 0], [1783, 1.0, 0], [1784, 1.0, 0], [1785, 1.0, 0], [1792, 1.0, 0], [1794, 1.0, 0], [1797, 1.0, 0], [1800, 1.0, 0], [1802, 1.0, 0], [1803, 1.0, 0], [1805, 1.0, 0], [1812, 1.0, 0], [1816, 1.0, 0], [1825, 1.0, 0], [1826, 1.0, 0], [1830, 1.0, 0], [1835, 1.0, 0], [1839, 1.0, 0], [1848, 1.0, 0], [1850, 1.0, 0], [1857, 1.0, 0], [1859, 1.0, 0], [1861, 1.0, 0], [1863, 1.0, 0], [1867, 1.0, 0], [1874, 1.0, 0], [1878, 1.0, 0], [1882, 1.0, 0], [1898, 1.0, 0], [1901, 1.0, 0], [1903, 1.0, 0], [1906, 1.0, 0], [1908, 1.0, 0], [1910, 1.0, 0], [1913, 1.0, 0], [1914, 1.0, 0], [1915, 1.0, 0], [1916, 1.0, 0], [1921, 1.0, 0], [1924, 1.0, 0], [1929, 1.0, 0], [1930, 1.0, 0], [1932, 1.0, 0], [1935, 1.0, 0], [1939, 1.0, 0], [1941, 1.0, 0], [1946, 1.0, 0], [1951, 1.0, 0], [1956, 1.0, 0], [1959, 1.0, 0], [1961, 1.0, 0], [1969, 1.0, 0], [1972, 1.0, 0], [1980, 1.0, 0], [1981, 1.0, 0], [1982, 1.0, 0], [1986, 1.0, 0], [1988, 1.0, 0], [2017, 1.0, 0], [2020, 1.0, 0], [2023, 1.0, 0], [2024, 1.0, 0], [2028, 1.0, 0], [2039, 1.0, 0], [2049, 1.0, 0], [2056, 1.0, 0], [2057, 1.0, 0], [2058, 1.0, 0], [2061, 1.0, 0], [2063, 1.0, 0], [2064, 1.0, 0], [2067, 1.0, 0], [2074, 1.0, 0], [2077, 1.0, 0], [2083, 1.0, 0], [2099, 1.0, 0], [2103, 1.0, 0], [2105, 1.0, 0], [2109, 1.0, 0], [2112, 1.0, 0], [2115, 1.0, 0], [2119, 1.0, 0], [2123, 1.0, 0], [2125, 1.0, 0], [2126, 1.0, 0], [2132, 1.0, 0], [2134, 1.0, 0], [2137, 1.0, 0], [2140, 1.0, 0], [2141, 1.0, 0], [2143, 1.0, 0], [2148, 1.0, 0], [2151, 1.0, 0], [2153, 1.0, 0], [2155, 1.0, 0], [2157, 1.0, 0], [2162, 1.0, 0], [2164, 1.0, 0], [2175, 1.0, 0], [2178, 1.0, 0], [2188, 1.0, 0], [2199, 1.0, 0], [2202, 1.0, 0], [2205, 1.0, 0], [2206, 1.0, 0], [2216, 1.0, 0], [2217, 1.0, 0], [2223, 1.0, 0], [2228, 1.0, 0], [2232, 1.0, 0], [2235, 1.0, 0], [2240, 1.0, 0], [2247, 1.0, 0], [2249, 1.0, 0], [2250, 1.0, 0], [2254, 1.0, 0], [2262, 1.0, 0], [2265, 1.0, 0], [2272, 1.0, 0], [2274, 1.0, 0], [2291, 1.0, 0], [2292, 1.0, 0], [2300, 1.0, 0], [2306, 1.0, 0], [2310, 1.0, 0], [2311, 1.0, 0], [2313, 1.0, 0], [2316, 1.0, 0], [2320, 1.0, 0], [2323, 1.0, 0], [2329, 1.0, 0], [2335, 1.0, 0], [2336, 1.0, 0], [2341, 1.0, 0], [2342, 1.0, 0], [2351, 1.0, 0], [2356, 1.0, 0], [2368, 1.0, 0], [2376, 1.0, 0], [2385, 1.0, 0], [2387, 1.0, 0], [2389, 1.0, 0], [2391, 1.0, 0], [2394, 1.0, 0], [2395, 1.0, 0], [2402, 1.0, 0], [2408, 1.0, 0], [2410, 1.0, 0], [2413, 1.0, 0], [2415, 1.0, 0], [2417, 1.0, 0], [2419, 1.0, 0], [2420, 1.0, 0], [2422, 1.0, 0], [2425, 1.0, 0], [2427, 1.0, 0], [2429, 1.0, 0], [2435, 1.0, 0], [2436, 1.0, 0], [2439, 1.0, 0], [2443, 1.0, 0], [2445, 1.0, 0], [2452, 1.0, 0], [2455, 1.0, 0], [2458, 1.0, 0], [2459, 1.0, 0], [2467, 1.0, 0], [2472, 1.0, 0], [2478, 1.0, 0], [2480, 1.0, 0], [2481, 1.0, 0], [2482, 1.0, 0], [2488, 1.0, 0], [2497, 1.0, 0], [2506, 1.0, 0], [2513, 1.0, 0], [2514, 1.0, 0], [2516, 1.0, 0], [2519, 1.0, 0], [2520, 1.0, 0], [2522, 1.0, 0], [2527, 1.0, 0], [2529, 1.0, 0], [2532, 1.0, 0], [2540, 1.0, 0], [2547, 1.0, 0], [2548, 1.0, 0], [2549, 1.0, 0], [2562, 1.0, 0], [2566, 1.0, 0], [2567, 1.0, 0], [2569, 1.0, 0], [2570, 1.0, 0], [2571, 1.0, 0], [2574, 1.0, 0], [2576, 1.0, 0], [2581, 1.0, 0], [2582, 1.0, 0], [2584, 1.0, 0], [2588, 1.0, 0], [2594, 1.0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(index_label_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
