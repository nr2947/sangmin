{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 를 이용하여 data-01.csv 선형회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class LinearRegressionTest:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, xdata, tdata, learning_rate, iteration_count):\n",
    "        \n",
    "        self.xdata = xdata\n",
    "        self.tdata = tdata\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.iteration_count = iteration_count\n",
    "        \n",
    "        self.W = np.random.rand(self.xdata.shape[1], 1)   # 입력 xdata가 이미 행렬이라 가정한 구현\n",
    "        self.b = np.random.rand(1)\n",
    "        \n",
    "        print(\"LinearRegressionTest Object is created\")\n",
    "        \n",
    "    \n",
    "    # obtain current W and current b\n",
    "    def getW_b(self):\n",
    "        \n",
    "        return self.W, self.b\n",
    "    \n",
    "    \n",
    "    # loss function\n",
    "    def loss_func(self):\n",
    "        \n",
    "        y = np.dot(self.xdata, self.W) + self.b\n",
    "    \n",
    "        return ( np.sum( (self.tdata - y)**2 ) ) / ( len(self.xdata) )\n",
    "        \n",
    "    \n",
    "    # display current error value\n",
    "    def error_val(self):\n",
    "        \n",
    "        y = np.dot(self.xdata, self.W) + self.b\n",
    "    \n",
    "        return ( np.sum( (self.tdata - y)**2 ) ) / ( len(self.xdata) )\n",
    "    \n",
    "    \n",
    "    # predict method\n",
    "    def predict(self, test_data):\n",
    "        \n",
    "        y = np.dot(test_data, self.W) + self.b\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    # train method\n",
    "    def train(self):\n",
    "    \n",
    "        f = lambda x : self.loss_func()\n",
    "\n",
    "        print(\"Initial error value = \", self.error_val(), \"Initial W = \", self.W, \"\\n\", \", b = \", self.b )\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        for step in  range(self.iteration_count):  \n",
    "    \n",
    "            self.W -= self.learning_rate * numerical_derivative(f, self.W)\n",
    "    \n",
    "            self.b -= self.learning_rate * numerical_derivative(f, self.b)\n",
    "    \n",
    "            if (step % 400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val(), \"W = \", self.W, \", b = \", self.b )\n",
    "                \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.ndim =  2 , x_data.shape =  (25, 3)\n",
      "t_data.ndim =  2 , t_data.shape =  (25, 1)\n"
     ]
    }
   ],
   "source": [
    "loaded_data = np.loadtxt('./data-01.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[ :, 0:-1]\n",
    "t_data = loaded_data[ :, [-1]]\n",
    "\n",
    "# 데이터 차원 및 shape 확인\n",
    "print(\"x_data.ndim = \", x_data.ndim, \", x_data.shape = \", x_data.shape)\n",
    "print(\"t_data.ndim = \", t_data.ndim, \", t_data.shape = \", t_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressionTest Object is created\n",
      "Initial error value =  84.63652300892261 Initial W =  [[0.60678534]\n",
      " [0.55805717]\n",
      " [0.95423434]] \n",
      " , b =  [0.73291963]\n",
      "step =  0 error value =  36.04825940697643 W =  [[0.59270016]\n",
      " [0.54393095]\n",
      " [0.93986959]] , b =  [0.73281259]\n",
      "step =  400 error value =  7.247863108465246 W =  [[0.55006365]\n",
      " [0.51556326]\n",
      " [0.94409538]] , b =  [0.73209621]\n",
      "step =  800 error value =  7.0223158002270365 W =  [[0.53124817]\n",
      " [0.5108518 ]\n",
      " [0.96699247]] , b =  [0.73151421]\n",
      "step =  1200 error value =  6.850057317392089 W =  [[0.51428035]\n",
      " [0.5075539 ]\n",
      " [0.98671285]] , b =  [0.73090672]\n",
      "step =  1600 error value =  6.717705910835184 W =  [[0.49897501]\n",
      " [0.50537785]\n",
      " [1.00372154]] , b =  [0.73027724]\n",
      "step =  2000 error value =  6.615419825340862 W =  [[0.4851661 ]\n",
      " [0.50408529]\n",
      " [1.01841276]] , b =  [0.72962878]\n",
      "step =  2400 error value =  6.535922046124015 W =  [[0.47270467]\n",
      " [0.50348201]\n",
      " [1.03112106]] , b =  [0.72896391]\n",
      "step =  2800 error value =  6.473801675192469 W =  [[0.46145701]\n",
      " [0.5034102 ]\n",
      " [1.04213056]] , b =  [0.72828483]\n",
      "step =  3200 error value =  6.425011906435983 W =  [[0.45130302]\n",
      " [0.5037421 ]\n",
      " [1.0516828 ]] , b =  [0.72759343]\n",
      "step =  3600 error value =  6.386508021208076 W =  [[0.44213481]\n",
      " [0.50437468]\n",
      " [1.05998331]] , b =  [0.72689134]\n",
      "step =  4000 error value =  6.355985350154213 W =  [[0.43385535]\n",
      " [0.50522524]\n",
      " [1.06720718]] , b =  [0.72617995]\n",
      "step =  4400 error value =  6.331688799576459 W =  [[0.42637738]\n",
      " [0.50622777]\n",
      " [1.0735037 ]] , b =  [0.72546046]\n",
      "step =  4800 error value =  6.31227376631737 W =  [[0.41962241]\n",
      " [0.50732989]\n",
      " [1.07900033]] , b =  [0.72473393]\n",
      "step =  5200 error value =  6.296704078668775 W =  [[0.41351974]\n",
      " [0.50849041]\n",
      " [1.08380602]] , b =  [0.72400124]\n",
      "step =  5600 error value =  6.284176715106662 W =  [[0.40800574]\n",
      " [0.50967722]\n",
      " [1.08801399]] , b =  [0.72326318]\n",
      "step =  6000 error value =  6.27406596893491 W =  [[0.40302307]\n",
      " [0.51086557]\n",
      " [1.09170412]] , b =  [0.72252042]\n",
      "step =  6400 error value =  6.26588179770223 W =  [[0.39852009]\n",
      " [0.51203671]\n",
      " [1.09494493]] , b =  [0.72177355]\n",
      "step =  6800 error value =  6.259238569650278 W =  [[0.39445023]\n",
      " [0.5131767 ]\n",
      " [1.09779529]] , b =  [0.72102307]\n",
      "step =  7200 error value =  6.253831470197652 W =  [[0.39077151]\n",
      " [0.51427546]\n",
      " [1.10030585]] , b =  [0.72026943]\n",
      "step =  7600 error value =  6.249418582743874 W =  [[0.38744607]\n",
      " [0.51532599]\n",
      " [1.10252023]] , b =  [0.71951301]\n",
      "step =  8000 error value =  6.2458071967972035 W =  [[0.38443976]\n",
      " [0.51632375]\n",
      " [1.10447606]] , b =  [0.71875416]\n",
      "step =  8400 error value =  6.242843283984607 W =  [[0.38172176]\n",
      " [0.51726612]\n",
      " [1.10620586]] , b =  [0.71799316]\n",
      "step =  8800 error value =  6.240403362309256 W =  [[0.37926426]\n",
      " [0.51815194]\n",
      " [1.10773775]] , b =  [0.71723028]\n",
      "step =  9200 error value =  6.238388171836197 W =  [[0.37704217]\n",
      " [0.51898125]\n",
      " [1.10909611]] , b =  [0.71646575]\n",
      "step =  9600 error value =  6.236717732629522 W =  [[0.37503282]\n",
      " [0.51975493]\n",
      " [1.11030209]] , b =  [0.71569976]\n",
      "step =  10000 error value =  6.235327463747326 W =  [[0.37321576]\n",
      " [0.52047453]\n",
      " [1.11137407]] , b =  [0.71493249]\n",
      "\n",
      "Elapsed Time =>  0:00:01.085371\n"
     ]
    }
   ],
   "source": [
    "# LinearRegressionTest 객체를 만들기 위해 4개의 파라미터 필요\n",
    "# 1st : 입력데이터,  2nd : 정답데이터\n",
    "# 3rd : learning rate,  4th : iteration count\n",
    "obj = LinearRegressionTest(x_data, t_data, 1e-5, 10001)\n",
    "\n",
    "obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.29276896])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100, 98, 81])\n",
    "\n",
    "obj.predict(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.39208095]\n",
      " [0.51154065]\n",
      " [1.10402201]] , b =  [0.52790653]\n"
     ]
    }
   ],
   "source": [
    "(W, b) = obj.getW_b()\n",
    "\n",
    "print(\"W = \", W, \", b = \", b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
